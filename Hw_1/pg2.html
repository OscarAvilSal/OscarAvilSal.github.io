<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="style.css" rel="stylesheet">
    <title>How?</title>
</head>

<header>
    <h1> How are they doing it? </h1>
</header>

<body>
    <a class="space" href="index.html"> Home </a>
    <a class="space" href="pg1.html"> Why? </a>
    <a class="space" href="pg2.html"> How? </a>
    <a class="space" href="pg3.html"> When? <br> <br> </a>

    <a class="write">
        These tools work by subtly altering an artwork in ways that are invisible or harmless to human viewers but confusing to AI training models. When an AI system scrapes the image and tries to learn from it, the hidden modifications cause the model to misinterpret what it’s seeing — for example, mistaking a dog for a cat or reading the style incorrectly. NPR explains that tools like Nightshade embed tiny pixel‑level changes that shift how an AI model maps visual features, so if the altered image is included in a training dataset, it “poisons” the model’s understanding of that category or style. Importantly, this doesn’t break existing models; it only affects future models trained on the modified images. The idea is to make unauthorized scraping less useful by ensuring that training on unlicensed art leads to distorted or unreliable results, giving artists a form of leverage in a system where consent has been difficult to enforce
        <br>
    </a>

    <a class="write"> bellow is a comparsion of AI art to real Art<br> </a>
    <a class="write"> . <br> . <br> . <br> . <br> \/ <br> <br></a>

    <img src="img/b5263d-2024-02-dos-11-webp1047.webp" width="1000" height="600" alt="Comparison image">
</body>
</html>