<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="style.css" rel="stylesheet">
    <title>Why? </title>
</head>
<header>
    <h1> Why are they doing this? </h1>

</header>
<body>
    <a class="space" href="index.html"> Home </a>
    <a class="space" href="pg1.html"> Why? </a>
    <a class="space" href="pg2.html"> How? </a>
    <a class="space" href="pg3.html"> When? <br> <br> </a>
    <a class="write">Artists sometimes talk about “poisoning AI” as a way to protect their work from being used in training datasets without permission. In reality, this refers to non‑harmful, defensive techniques like style‑confusing filters or watermark‑like signals that make artwork less useful for AI training. These methods don’t damage systems; 
            they simply help artists maintain control over how their art is used. <br><br><br>
    </a>
    <a class="write"> bellow is a comparsion of AI art to real Art<br> </a>
        <a class="write"> . <br> . <br> . <br> . <br> \/ <br> <br></a>
    <img src="img\image-89-2048x862.avif" width="1000" height="600">
</body>
</html>